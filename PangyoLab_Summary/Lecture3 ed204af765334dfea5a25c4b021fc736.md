# Lecture3

### Dynamic programming

Dynamic programming (이하 DP)는 어려운 문제를 작은 단위로 나눠서 푸는 방법이다. 작은 단위 문제(subproblem)을 풀고 솔루션과 subproblem을 묶어서 결과를 도출 한다. 이때 몇 특징을 가진 문제들에 자주 사용되는데,

1. Optimal substructure
    1. principle of optimality가 적용된다.
    2. 최적의 솔루션이 subproblem 안에 나눠 들어가 있다.
2. Overlapping subproblem
    1. subproblem이 여러번 반복 된다.
    2. 해는 캐시처럼 저장되고 다시 사용가능하다.
3. Markov decision processes satisfy both properties
    1. 벨만식이 재귀적으로 나눠져야한다.
    2. Value 함수가 해를 저장하고 재 사용할 수 있다.

DP의 경우 MDP(Markov Decision Process)에 대한 지식을 모두 가지고 있어야한다. MDP안에서 경로를 탐색하는데, Prediction과 Contro 부분으로 나눠져있다.

1. Prediction
    1. input : MDP<S,A,P,R,$\gamma$> 과 policy $\pi$ 혹은 MRP <S,$P^{\pi}, R^{\pi},\gamma$>
    2. output : value 함수 $v_{\pi}$
2. Control
    1. input : MDP <S,A,P,R,$\gamma$>
    2. output : optimal value function $v_*$ 과 optimal policy $\pi_*$

### Iterative Policy Evaluation

Policy에 대한 평가를 해야 하는 문제이다. 평가한다는 것은 policy를 따라서 행동 했을 때 return을 얼마나 받는지에 대한 평가이다. 이는 벨만 기대 값 백업을 반복하여 적용하는 것으로 해결할 수 있다. 초기 value 값인 v들을 가지고 v1, v2, v3,,,,v_pi까지 알아가면 된다. 

backup에는 synchronous와 asynchronous가 있는데 여기서는 synchronous를 사용한다. 모든 state들의 value 값을 각각 다음 state값을 사용하여 업데이트 해준다. 이렇게 모든 state를 업데이트 해줬다면 다음번으로 넘어간다. 이렇게 한번에 전부 해주는 것을 synchronous라고 한다. 이를 n번하게 되면 

$v_{\pi}$에 수렴하게 된다.

![Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled.png](Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled.png)

$v_{k+1}(s) =\sum \pi(a|s)(R_s^a+\gamma\sum P_{ss'}v_k(s'))$

$v^{k+1}=R^{\pi}+\gamma P^{\pi}v^k$

그림에서의 Vk+1(s)의 값을 정확하게 하고 싶다고 하자. 그러면 s에서 갈 수 있는 state들은 총 4개가 있으니 이를 이용해서 Vk+1(s) 값을 정확하게 해준다. 따라서 식을 보게 되면 Reward와 discount가 된 state transition matrix *value function (Bellman equation)로 이뤄진 걸 알 수 있다.

### Improving a Policy

주어진 policy $\pi$에 대해서 먼저 **Evaluate**을 하고 **Improve**를 한다. 

이때 Evaluate은 앞서 말한 벨만식을 사용해서 하고, 

$v_{\pi}(s)=E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]$

Improve의 경우 $v_{\pi}$를 기준으로 greedy하게 움직이면 된다. 

$\pi'=greedy(v_{\pi})$

$\pi'$의 경우 많이 반복할 경우 $\pi^*$로 수렴하게 된다.

![Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled%201.png](Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled%201.png)

![Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled%202.png](Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled%202.png)

앞서 말한 것처럼 greedy하게 행동하여 policy를 imporve할 수 있다고 하는데 정말 그럴까?

$\pi'=\argmax q_{\pi}(s,a)$

먼저 state s에서 policy $\pi$를 따를 때 value $v_{\pi}(s)$가 있다고 하자. 이는 state s에서 policy $\pi$로 action을 하나 고르는 action value 함수인 $q_{\pi}(s,a)$와 같다. 이 값은 state s에서 할 수 있는 모든 action value 값들 보다는 작거나 같을 것이다. 

즉,  $\max q_{\pi}(s,a)>=q_{\pi}(s,\pi(s))=v_{\pi}(s)$인데, max값은 greedy policy를 따랐을 때 q값이 된다.

$q_{\pi}(s,\pi'(s))=\max q_{\pi}(s,a)>=q_{\pi}(s,\pi(s))=v_{\pi}(s)$

이는 one step에서 greedy한 방법으로 improve한 policy를 따르는 값이 더 크다는 것을 이야기하고

이를 반복 적으로 하게 되면 결국 improve한 policy가 가장 큰 value function값을 가지게 한다고 보일 수 있다. 따라서 improvement가 끝나게 되면

$q_{\pi}(s,\pi'(s))=\max q_{\pi}(s,a)>=q_{\pi}(s,\pi(s))=v_{\pi}(s)$ 이 되게 되고 Bellman optimality equation이 만족되게 된다.

$v_{\pi}(s)=\max q_{\pi}(s,\pi(s))$ , 따라서 $v_{\pi}(s)=v_*(s)$이 되며 ${\pi}$가 최적 policy가 된다.

$v_{\pi}(s)=\max q_{\pi}(s,\pi(s))$

$v_{\pi}(s)=v_*(s)$dd

### Modified Policy Iteration

그렇다면 policy evaluation은 꼭 $v_\pi$로 수렴해야 되는 걸까? 아니면 중간에 멈춰서 policy를 평가해도 되는걸까? 실제로 반복 횟수(k)를 100으로 해도 1로 해도 상관없다. 이에 따른 방법들이 존재한다.

### Principle of optimality

기존에 subproblem을 cahce 혹은 store해서 최종 해를 구하는 방법에 대해서 이야기 했었다.

이 방법을 사용해 s'에서 value값을 알면 one-step lookahead를 통해 최적 s의 value 값을 알 수 있다.

식으로 표현하면 아래와 같다.

$v_*(s)\leftarrow (\max R_s^a+\gamma \sum P_{ss'}v_*(s'))$

이 식은 Bellman optimality equation인데, Bellman 식에는 optimality와 expectation 식이 있는데 expectation은 optimality식과 다르게 역행렬로 넘겨서 해를 구할 수 있다.(리마인드)

### Deterministic Value Iteration

value iteration은 앞서 말한 policy iteration과 다른 내용이다. value iteration에는 정해진 policy가 없이 value 값만 iteration 한다. value iteration이 가지는 problem은 optimal policy를 찾는 것이다.

그에 대한 solution은 Bellman optimality backup을 반복 적으로 적용하는 것이다.

이때도 모든 state들을 각 iteration마다 update하는 synchronous backup을 사용했다. 

### Synchronous DP Algorithms

![Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled%203.png](Lecture3%20ed204af765334dfea5a25c4b021fc736/Untitled%203.png)

### Asynchronous DP

DP 알고리즘들은 모두 synchronous backup을 사용한다. 즉 모든 state들이 모두 업데이트 되는 것이다. 하지만 asynchronous의 경우 여러 방식으로 state들을 update할 수 있는데, 이는 계산량을 매우 줄일 수 있다.  state들이 반복적으로 여러 번 뽑히게 되면 수렴성도 보장 된다.

asynchronous dp를 구현하는 방법으로는 3가지가 있다.

- Inplace dynamic programming

    : value function 업데이트 이전 값과 이후 값을 저장하는 것이 아닌, 업데이트 된 값 하나만 저장하는 방법

- Prioritised sweeping

    : state를 업데이트 함에 있어서, 단순히 업데이트 하기만 하면 상관없지만, 중요한 state들을 우선으로 업데이트 하는 방법. 중요한 state들은 Bellman error가 컸던 state가 중요한 state가 된다.

- Real-time dynamic programming

    : state space가 매우 큰데, agent가 가는 state를 먼저 업데이트 하는 방법이다. 

이 외에도 2개의 방법을 더 설명해보자면

- Full Width Backup

    : DP는 state를 구하기 위해 이전 state들을 모두 참고하여 update를 하는데, state들이 매우 많아지면 full width backup을 할 수 없게 된다. 이를 curse of dimensionality라고 한다.

- Sample Backup

    : 차원의 저주를 피하기 위해 나타난 방법이다. 차원이 커지고, 모델이 없어도 사용 가능하다.

    일단 state에서 어떤 action을 취해서 sample 100개를 가지고 state를 업데이트 해준다.