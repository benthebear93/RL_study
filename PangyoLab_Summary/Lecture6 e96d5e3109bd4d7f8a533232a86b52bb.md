# Lecture6

### Large Scale RL

앞서 이야기한 방법들은 Table lookup, state들에 맞게 table을 만들어 놓고 방문, 업데이트 하는 형식이었다. 하지만 실제 문제의 경우 state가 매우 많은 경우가 대다수 인데 단순 backgammon이라는 보드게임은 10^20의 state가 바둑은 10^170 state가 있다. 이를 Model free 방법에서 scale up하기 위해서는 value function approximation이 필요하다.

### Value function approximation

매우 큰 MDP는 각 Value, action function들을 근사해주는 방법으로 해결 할 수 있다.

$\hat{v}(s,w)\approx v_\pi(s)$ or $\hat{q}(s,a,w)\approx q_\pi(s,a)$

w는 근사 된 v안에 들어가 있는 파라미터들을 뜻한다. w은 함수의 입력값을 뜻한다기 보다는 단순 파라미터라는 걸 알고가자. 이때 vfa의 경우는 보지 않은 state들도 일반화를 잘 한다고 한다. 자세한 내용은 뒤에서 조금 더 설명하겠다.

### Types of Value Function Approximation

![Lecture6%20e96d5e3109bd4d7f8a533232a86b52bb/Untitled.png](Lecture6%20e96d5e3109bd4d7f8a533232a86b52bb/Untitled.png)

근사에 대한 방법들은 3가지의 블랙박스로 나타낼 수 있다.

s와 (s,a), s를 넣었을 때인데 정리하면 아래와 같다. 

1. value 함수로 나오는 경우
2. action value 함수로 나오는 경우(action in 형태)
3. s에 대해 모든 경우의 action value 함수로 나오는 경우(action out형태)

### Which Function Approximator?

- Linear combination of features
- NN
- Decision Tree
- Nearest neighbour
- Fourier/ wavelet bases

이 중에서 미분 가능한 근사 함수를 사용한다. 미분을 할 수 있어야 gradient를 구하여 w 파라미터들을 업데이트 할 수 있기 때문이다. 

### Incremental Method-GD

근사 함수가 하려는 일은 value 함수를 근수하는 작업이다. 여기서 잘 근사, 혹은 일반화가 됐다면 그 둘의 차이는 작다고 생각할 수 있다. 이를 J 함수로 나타내면 $J(w)=E_\pi[(v_\pi(S)-\hat{v}(S,w))^2]$ 가 된다.

이때 실제 value 함수는 모르지만 안다고 가정하고 식을 정리한다.

경사하강법 방향으로 w를 표현하게 되면 아래와 같다.

$\bigtriangleup w=-0.5\alpha \bigtriangledown_w J(w) \\
\bigtriangleup w=\alpha (v_{\pi}(S)-\hat{v}(S,w))\bigtriangledown_w \hat{v}(S,w)$

$$\bigtriangleup w=-0.5\alpha \bigtriangledown_w J(w) \\
\bigtriangleup w=\alpha (v_{\pi}(S)-\hat{v}(S,w))\bigtriangledown_w \hat{v}(S,w)$$

### Feature Vectors

state를 인풋으로 넣을 때 나올 수 있는 다양한 feature들을 넣는다. 예를 들어서 로봇으로 부터 랜드마크 까지의 거리, 주식 시장의 트랜드 등. 

이 feature들을 이용해 근사한 value function을 표현하게 되면 아래 식과 같아진다.

$\hat{v}(S,w)=x(S)^Tw=\sum x_j(S)w_j$

feature와 w 파라미터의 곱으로 근사 value function를 만드는 것이다. 이때 목적 함수(Objective function)의 경우 실제 value function과 x와 w를 내적한 값의 차이와 연관이 있다.

식으로 표현하면 아래와 같다.

$J(w)=E_{\pi}[(v_{\pi}(s)-x(S)^Tw)^2]$

이때 근사함수는 global한 최적의 값으로 수렴하는데, 이는 함수가 convex하기 때문이다. 

이제 update 규칙은 $\bigtriangledown_w \hat{v}(S,w) =x(S)$, $\bigtriangleup w=\alpha(v_\pi(S)-\hat{v}(S,w))x(S)$

즉, Update = 스텝사이즈 x 예측 오류 x feature value

### Table Lookup Features(?)

Table lookup 방법은 사실 linear value function approximation의 한 예시일 뿐 이었다. 

### Incremental Prediction Algorithms

앞선 설명들에서는 $v_\pi(s)$를 알고 있다고 하고 설명을 했었는데, 실제로는 모르는 상태이다. 따라서 true value 값이 들어가야 할 자리에 MC, TD를 사용해서 return G와 TD target을 넣어주면 된다. 조금 생각해보면, 우리는 특정 St,에 대해서 output Gt를 원하고 이 방항으로 업데이트 하기를 원하니 MC 방식을 사용하여 얻을 수 있는 Gt를 사용할 수 있다. 

### MC with Value Function Approximation

MC의 Return G를 실제 value function 대신 사용한다고 했다. return은 reward들이 discount되서 합쳐진 값이다. value function의 unbiased한데, 그 이유는 value값은 return의 기대값인데 에피소드를 충분히 많이 하게 되면 return값으로 수렴하기 때문이다. 

따라서 unbiased되어 있기 때문에 return을 사용해서 update해도 무방하다. 이는 supervised learning에서 training data로 사용해도 된다라고 볼 수 있다. 

### TD Learning with Value Function Approximation

TD의 경우 추측치에 기반해서 Update하기 때문에 unbiased 하지는 않지만, 다행이 linear TD(0)는 global 최적값에 가깝게 수렴한다. 

### Control with value function approximation

control의 경우 2가지 스탭이 있다. Policy evaluation과 policy improvement. Model free에서는 evaluation에 q함수를 improvement에서는 입실론 그리디 방법을 사용한다. 

여기서 q 함수를 predict에서 value function을 approximate한 것처럼 근사하여 evaluation에 사용하면 되지 않을까? 가 이 부분의 핵심이다. 

value function을 했던 것과 마찬가지로 근사하고 실제 값과 차이를 목적함수로 만들고, 이를 미분해서 local minimum 값을 찾으면 된다.

$\hat{q}(S,A,w)\approx q_\pi(S,A)$

$J(w)=E_{\pi}[(q_{\pi}(s)-\hat{q}(S,A,w))^2]$

### Linear Sarsa

policy evaluation → policy improvement 에서 linear를 통해 approximation 한 근사 함수들을 사용했다면 이를 Linear Sarsa 방법이라고 한다.

### Batch Reinforcement Learning

Batch 방법은 앞에서 이야기 했던 Incremental 방법이 경험들을 한번 사용하고 버리기 때문에 이를 해결하기 위해 나온 방법이다. off policy와도 비슷하다고 볼 수 있다.

주어진 $\hat{v}(s,w)\approx v_\pi(s)$ 근사 value 함수에서 경험 D는 아래와 같이 표현된다.

$D=[<s_1,v_1^\pi>,<s_1,v_1^\pi>,....,<s_T,v_T^\pi>]$

이때도 똑같에 근사 value함수를 가장 잘 fitting할 w 파라미터를 찾고 싶어한다. 다른 방법과 동일하게 Least square 방법론으로 사용한다. 

$LS(w)=\sum(v_t^\pi-\hat{v}(s_t,w))^2=E_D[(v^\pi-\hat{v}(s,w))^2]$

### Experience Replay

여기서 다른 방법들과 다른점은 LS에서 기대값이 D를 따라갔을 때 즉 경험을 따라갔을 때의 기대값이다. 이는 state와 value값의 pair로 이루어진 경험 D에서 sampling을 하여 update한다.