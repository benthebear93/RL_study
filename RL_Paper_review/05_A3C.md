**논문 제목 : Policy gradient methods for reinforcement learning with function approximation Policy Gradient Algorithms**

**논문링크 :** **http://proceedings.mlr.press/v48/mniha16.pdf**

**https://github.com/benthebear93/RL_practice** **(추후 공개예정)**

****

**[Basic Idea]**

 이번 논문은 policy based 논문인 Asynchronous method for DRL이다. 

actor는 환경과 에이전트가 같이 있어서 경험을 쌓는 것이고, learner는 학습 시키는 부분을 말한다. 논문에서는  actor-learner 가 병렬로 이루어져서 진행된다. 싱크를 맞추는 방법은 정해진 주기 만큼 actor가 경험을 쌓고 그  경험을 글로벌 네트워크에 전달한다. 이때 모든 actor - learner는 글로벌네트워크를 공유하고 있다. 경험을 전달받은  네트워크는 다시 업데이트하셔 네트워크를 각 actor-learner로 보내준다. 이를 반복하면서 학습한다. 이때  Asynchronous란 각 쓰레드가 동일한 시간에 동시에 업데이트 하는 것이 아니라 각자 다른 시간에 업데이트하여 글로벌  네트워크게 공유하는 방식이다. 



(https://arxiv.org/pdf/1610.00633.pdf, Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. 이 논문도 비슷한 내용이었는데 다른 게 뭘까?)



 이때 헷갈릴 수 있는 부분은 논문에서 언급하는 내용은 범용적인 비동기적 학습 방법에 대해서 이야기하는 것이다. 따라서 쓰레드에 적용되는 알고리즘이 특정 알고리즘으로 국한되지 않는다. 



 또한 각 쓰레드에서 사용되는 파라미터 혹은 변수(?) 들을 다양하게 설정할 수도 있다. 특정 actor가 더 exploration을 많이 하고, 특정 actor는 조금 보수적으로 하는 식의 차이를 줄 수 있다. 



**[Introduction]**

과거에는 online RL과 deep neural network간에 조합은 기본적으로 불안정하다고 생각했었다. 따라서 안정성을 높이기  위한 여러 방법들이 제시 됐었는데, 기본적으로 online RL을 통해서 얻어진 데이터들은 non-stationary하고  online RL의 업데이트는 순차적인 정보들이기 때문에 데이터들의 상관관계(correlation)가 높다. 따라서 replay  memory에 데이터를 저장하여서 batch되게 만들거나 랜덤하게 샘플되게 만들었었다. 이렇게 데이터들은 모으면  non-stationary를 줄일 수 있고 업데이트를 decorrelate할 수 있다. 다만 이는 off-policy에만  적용가능하다는 단점이 있다.



논문에서의 기여하는 부분은 아래와 같이 정리할 수 있다.



**1. experience를 replay이 하는 것이 아닌, 비동기적으로 다수의 agent를 병렬로 돌려서 더욱 stationary한  프로세스로 에이전트의 데이터를 decorrelate한다.  (experience replay를 결합해서 쓰면 더 좋다는 이야기도  있다.)**

**2. on policy/ off policy , Value based/ Policy based 상관없이 모두 사용가능하다.**

**3. GPU 대신 멀티코어 CPU에서 돌릴 수 있다.** 

**4. Super Linear**



(1번 첨언 , 기본적으로 SL의 경우 IId(independent, identically distributed)를 가정하는데 RL의  경우 correlation이 매우 크기 때문에 불안정하다고 한다. 1번 방식의 경우 RL을 Supervised learning의  형태로 학습하기 위해 생각한 방법들이라고 한다. - 팡요랩)

(Super linear란 linear보다 좋은 느낌으로, 논문에서 언급하는 병렬 학습으로 학습 시간이 리니어하게 줄어드는 것이 아니라  리니어보다 더 많이 줄어든다는 뜻이다. 즉 4개로 학습시켰는데 4배 빨라지는게 아니라 6배 빨라진다거나 했다는 뜻이다)



**[Realated Work]**

생략



**[Reinforcement Learning Background]**

논문에서는 